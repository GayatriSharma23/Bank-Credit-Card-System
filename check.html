class CombinedTabNetTabTransformer(nn.Module):
    def __init__(self, num_features, num_classes, num_cat_features, num_cont_features, cat_dims):
        super(CombinedTabNetTabTransformer, self).__init__()
        
        # TabTransformer
        self.tab_transformer = TabTransformer(
            categories = num_cat_features,  # Number of categorical features
            num_continuous = num_cont_features,  # Number of continuous features
            dim = 32,  # Embedding dimension for transformer
            depth = 4,  # Number of transformer layers
            heads = 4,  # Number of attention heads
            attn_dropout = 0.1,
            ff_dropout = 0.1
        )
        
        # TabNet: Initialize separately, train separately, and then retrieve predictions
        self.tabnet = TabNetClassifier(
            input_dim=num_features,  # Total number of input features (categorical + continuous)
            n_d=16, n_a=16, n_steps=3, 
            gamma=1.3, n_independent=2, 
            n_shared=2, 
            cat_idxs=list(range(num_cat_features)),
            cat_dims=cat_dims,
            cat_emb_dim=1  # Adjust embedding size based on categorical features
        )
        
        # Final classifier after combining TabTransformer and TabNet outputs
        self.fc = nn.Linear(32 + 16, num_classes)  # Concatenate TabTransformer (32) + TabNet (16) outputs

    def forward(self, x_cat, x_cont, tabnet_output):
        # TabTransformer forward pass
        transformer_output = self.tab_transformer(x_cat, x_cont)

        # Combine TabTransformer and TabNet outputs
        combined_output = torch.cat([transformer_output, tabnet_output], dim=1)

        # Final classification layer
        output = self.fc(combined_output)
        return output

# Sample Data Preparation (replace with actual data)
data = pd.DataFrame({
    'feature1': np.random.rand(1000),
    'feature2': np.random.rand(1000),
    'feature3': np.random.rand(1000),
    'cat_feature1': np.random.randint(0, 5, size=1000),  # Categorical feature
    'cat_feature2': np.random.randint(0, 3, size=1000),  # Categorical feature
    'target': np.random.randint(0, 2, size=1000)  # Binary target
})

# Assume 'cat_feature1' and 'cat_feature2' are categorical columns
cat_features = ['cat_feature1', 'cat_feature2']
cont_features = ['feature1', 'feature2', 'feature3']
target = 'target'

# TabNet requires info about categorical columns (indices and dimensions)
cat_dims = [data[col].nunique() for col in cat_features]

# Split the data into categorical, continuous, and target
X_cat = data[cat_features].values
X_cont = data[cont_features].values
y = data[target].values

# Train-test split
X_cat_train, X_cat_test, X_cont_train, X_cont_test, y_train, y_test = train_test_split(
    X_cat, X_cont, y, test_size=0.2, random_state=42
)

# Initialize Model
num_cat_features = len(cat_features)
num_cont_features = len(cont_features)
num_features = num_cat_features + num_cont_features  # Total features (categorical + continuous)
num_classes = 2  # Binary classification

# Train TabNet separately
tabnet = TabNetClassifier(
    input_dim=num_features,
    cat_idxs=[0, 1],  # Specify categorical feature indices
    cat_dims=cat_dims,
    cat_emb_dim=1
)

# Prepare inputs for TabNet
X_train = np.hstack([X_cat_train, X_cont_train])
X_test = np.hstack([X_cat_test, X_cont_test])

# Fit TabNet model
tabnet.fit(
    X_train, y_train,
    eval_set=[(X_test, y_test)],
    eval_name=["val"],
    eval_metric=["accuracy"],
    max_epochs=50,
    patience=10
)

# Get TabNet predictions (logits/probabilities)
tabnet_train_output = torch.FloatTensor(tabnet.predict_proba(X_train))
tabnet_test_output = torch.FloatTensor(tabnet.predict_proba(X_test))

# Combined model with TabTransformer and TabNet
model = CombinedTabNetTabTransformer(
    num_features=num_features,
    num_classes=num_classes,
    num_cat_features=num_cat_features,
    num_cont_features=num_cont_features,
    cat_dims=cat_dims
)

# Loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# Training loop for the combined model
num_epochs = 50
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    
    # Convert training data to tensor
    X_cat_tensor = torch.LongTensor(X_cat_train)
    X_cont_tensor = torch.FloatTensor(X_cont_train)
    y_tensor = torch.LongTensor(y_train)
    
    # Forward pass through combined model using TabNet predictions as input
    outputs = model(X_cat_tensor, X_cont_tensor, tabnet_train_output)
    loss = criterion(outputs, y_tensor)
    
    # Backward and optimize
    loss.backward()
    optimizer.step()

    if (epoch+1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

# Evaluation on test set
model.eval()
with torch.no_grad():
    X_cat_test_tensor = torch.LongTensor(X_cat_test)
    X_cont_test_tensor = torch.FloatTensor(X_cont_test)
    
    # Forward pass using TabNet predictions on test data
    test_outputs = model(X_cat_test_tensor, X_cont_test_tensor, tabnet_test_output)
    test_predictions = torch.argmax(test_outputs, dim=1).numpy()

# Metrics
print("F1 Score (Test):", f1_score(y_test, test_predictions))
print("Accuracy (Test):", accuracy_score(y_test, test_predictions))
Key Changes:
